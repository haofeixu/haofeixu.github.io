<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haofei Xu</title>
  
  <meta name="author" content="Haofei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="assets/fly.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- Bio -->
          <tr style="padding:0px">
            <td style="padding:2.5%;width:71%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haofei Xu </name> <name>(</name><span class="chinese">徐豪飞</span><name>)</name>
              </p>
              <p>
                I am a PhD student at <a href="https://ethz.ch/en.html">ETH Zurich</a> and <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/">University of Tübingen</a>, supervised by <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a> and <a href="http://www.cvlibs.net/">Andreas Geiger</a>. I am currently a Student Researcher at Google Zurich.
              </p>
              <p>
                I worked with <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a> at <a href="https://www.monash.edu/">Monash University</a>, Australia, prior to my PhD. I obtained a master's degree at <a href="http://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a> supervised by <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>. During my master's, I exchanged at <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a>, Singapore, where I was supervised by <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and <a href="https://scholar.google.com/citations?user=sGCf2k0AAAAJ">Jianmin Zheng</a>. I also interned at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>, where I was mentored by <a href="https://jlyang.org/">Jiaolong Yang</a> and <a href="https://scholar.google.com/citations?user=P91a-UQAAAAJ">Xin Tong</a>.
              </p>
              <p>
                I am honored to receive the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025">2025 Apple Scholar in AI/ML</a>, <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewer Award (NeurIPS 2024)</a>, and <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">Outstanding Reviewer Award (CVPR 2022)</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:haofei.xu@inf.ethz.ch">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NhUwq_8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://x.com/haofeixu">X</a> &nbsp/&nbsp
                <a href="https://bsky.app/profile/haofeixu.bsky.social">Bluesky</a> &nbsp/&nbsp
                <a href="https://github.com/haofeixu">Github</a>
                <!-- <a href="https://huggingface.co/haofeixu">Huggingface</a> -->
              </p>
            </td>
            <td style="padding: 10% 2% 10% 2%;width:40%;max-width:40%">
              <img src="assets/haofeixu.jpg" width="180" alt="photo">
            </td>
          </tr>

        </tbody></table>

        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <br>
              <br>
              I have broad interests in computer vision, particularly in fundamental research problems like dense correspondences, motion, 3D and video representation learning. I like to explore simple and effective approaches to solving fundamental challenges. Please see the full publication list on <a href="https://scholar.google.com/citations?user=NhUwq_8AAAAJ">Google Scholar</a>.
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- DepthSplat -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video autoplay muted loop width="180">
                <source src="./depthsplat/assets/video/re10k_f80f0077f529c4d7.mp4"
                        type="video/mp4">
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://haofeixu.github.io/depthsplat/">
                <papertitle>DepthSplat: Connecting Gaussian Splatting and Depth
                </papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://pengsongyou.github.io/">Songyou Peng</a>,
              <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a>,
              <a href="https://hermannblum.net/">Hermann Blum</a>,
              <a href="https://scholar.google.com/citations?user=U9-D8DYAAAAJ">Daniel Barath</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2410.13862">paper</a>
              /
              <a href="https://haofeixu.github.io/depthsplat/">project page</a>
              /
              <a href="https://github.com/cvg/depthsplat">code</a> 
              <iframe src="https://ghbtns.com/github-btn.html?user=cvg&repo=depthsplat&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
                Cross-task interactions between feed-forward Gaussian splatting and depth.
              </p>
            </td>
          </tr>

          <!-- PanSplat -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
              <img src='assets/pansplat.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://chengzhag.github.io/publication/pansplat/">
                <papertitle>PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting
                </papertitle>
              </a>
              <br>
              <a href="https://chengzhag.github.io/">Cheng Zhang</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://scholar.google.com/citations?user=gzCgAE8AAAAJ">Camilo Cruz Gambardella</a>,
              <a href="https://dinhphung.ml/">Dinh Phung</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2412.12096">paper</a>
              /
              <a href="https://chengzhag.github.io/publication/pansplat/">project page</a>
              /
              <a href="https://github.com/chengzhag/PanSplat">code</a> 
              /
              <a href="https://www.youtube.com/watch?v=9bKZA2zxAbw">interactive 4K 360 demo</a> 
              <p>
                4K panorama synthesis with a single feed-forward inference.
              </p>
            </td>
          </tr>

          <!-- NoPoSplat -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
              <video autoplay muted loop width="160">
                <source src="assets/noposplat.mp4"
                        type="video/mp4">
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://noposplat.github.io/">
                <papertitle>No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images
                </papertitle>
              </a>
              <br>
              <a href="https://botaoye.github.io/">Botao Ye</a>,
              <a href="https://sifeiliu.net/">Sifei Liu</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://sunshineatnoon.github.io/">Xueting Li</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
              <a href="https://pengsongyou.github.io/">Songyou Peng</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2410.24207">paper</a>
              /
              <a href="https://noposplat.github.io/">project page</a>
              /
              <a href="https://github.com/cvg/NoPoSplat">code</a> 
              <iframe src="https://ghbtns.com/github-btn.html?user=cvg&repo=NoPoSplat&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
                Unposed 3DGS reconstruction made easy.
              </p>
            </td>
          </tr>

          <!-- MVSplat360 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video autoplay muted loop width="180">
                <source src="assets/mvsplat360.mp4"
                        type="video/mp4">
              </video>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/mvsplat360">
                <papertitle>MVSplat360: Feed‑Forward 360 Scene Synthesis from Sparse Views
                </papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi">Andrea Vedaldi</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2411.04924">paper</a>
              /
              <a href="https://donydchen.github.io/mvsplat360">project page</a>
              /
              <a href="https://github.com/donydchen/mvsplat360">code</a> 
              <p>
                Empowering MVSplat with a video diffusion model.
              </p>
            </td>
          </tr>

          <!-- MVSplat -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/mvsplat.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://donydchen.github.io/mvsplat/">
                <papertitle>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images
                </papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2403.14627">paper</a>
              /
              <a href="https://donydchen.github.io/mvsplat/">project page</a>
              /
              <a href="https://github.com/donydchen/mvsplat">code</a> 
              <iframe src="https://ghbtns.com/github-btn.html?user=donydchen&repo=mvsplat&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
                A cost volume representation for efficiently predicting 3D Gaussians from sparse multi-view images in a single feed-forward inference.
              </p>
            </td>
          </tr>

          <!-- LaRa -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/lara.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://apchenstu.github.io/LaRa/">
                <papertitle>LaRa: Efficient Large-Baseline Radiance Fields
                </papertitle>
              </a>
              <br>
              <a href="https://apchenstu.github.io/">Anpei Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://s-esposito.github.io/">Stefano Esposito</a>,
              <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 
              <br>
              <a href="https://arxiv.org/abs/2407.04699">paper</a>
              /
              <a href="https://apchenstu.github.io/LaRa/">project page</a>
              /
              <a href="https://github.com/autonomousvision/LaRa">code</a>
              <p></p>
              <p>
                A feed-forward 2DGS model trained in two days using four GPUs.
              </p>
            </td>
          </tr>

          <!-- MuRF -->
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/murf.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.04565">
                <papertitle>MuRF: Multi-Baseline Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://apchenstu.github.io/">Anpei Chen</a>,
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://people.ee.ethz.ch/~csakarid/">Christos Sakaridis</a>,
              <a href="https://yulunzhang.com/">Yulun Zhang</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a><sup>&dagger;</sup>,
              <a href="https://www.yf.io/">Fisher Yu</a><sup>&dagger;</sup>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 
              <br>
              <a href="https://haofeixu.github.io/murf/">project page</a>
              /
              <a href="https://github.com/autonomousvision/murf">code</a>
              <p></p>
              <p>
                A general feed-forward approach to solving sparse view synthesis under multiple different baseline settings.
              </p>
            </td>
          </tr> -->

          <!-- GoMVS -->
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/gomvs.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.07992">
                <papertitle>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</papertitle>
              </a>
              <br>
              <a href="https://github.com/Wuuu3511">Jiang Wu</a>&#42;,
              <a href="https://ruili3.github.io/">Rui Li</a>&#42;,
              <strong>Haofei Xu</strong>,
              Wenxun Zhao,
              Yu Zhu,
              Jinqiu Sun,
              Yanning Zhang
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 
              <br>
              <a href="https://wuuu3511.github.io/gomvs/">project page</a>
              /
              <a href="https://github.com/Wuuu3511/GoMVS">code</a>
              <p></p>
              <p>
                Geometrically consistent matching cost aggregation with monocular normals. <br>
                <font color="red"><strong>1<sup>st</sup> place</strong></font> on <a href="https://www.tanksandtemples.org/leaderboard/AdvancedF/">Tanks and Temples (Advanced) leaderboard</a>.
              </p>
            </td>
          </tr> -->

          <!-- MatchNeRF -->
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/matchnerf.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2304.12294">
                <papertitle>Explicit Correspondence Matching for Generalizable Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>arXiv</em>, 2023 
              <br>
              <a href="https://donydchen.github.io/matchnerf/">project page</a>
              /
              <a href="https://github.com/donydchen/matchnerf">code</a>
              <p></p>
              <p>
                Employing explicit correspondence matching as a geometry prior enables NeRF to generalize across scenes.
              </p>
            </td>
          </tr> -->

          <!-- UniMatch -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/unimatch.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://haofeixu.github.io/unimatch/">
                <papertitle>Unifying Flow, Stereo and Depth Estimation</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=9jH5v74AAAAJ">Jing Zhang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a>,
              <a href="https://www.yf.io/">Fisher Yu</a>,
              <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ">Dacheng Tao</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2023 
              <br>
              <a href="https://arxiv.org/abs/2211.05783">paper</a>
              /
              <a href="https://haofeixu.github.io/unimatch/">project page</a>
              /
              <a href="./slides/20221228_synced_unimatch.pdf">slides</a>
              /
              <a href="https://www.bilibili.com/video/BV1uG4y1y7ms">video(cn)</a>
              /
              <a href="https://colab.research.google.com/drive/1r5m-xVy3Kw60U-m5VB-aQ98oqqg_6cab?usp=sharing">colab</a>
              /
              <a href="https://huggingface.co/spaces/haofeixu/unimatch">demo</a>
              /
              <a href="https://github.com/autonomousvision/unimatch">code</a>
              <iframe src="https://ghbtns.com/github-btn.html?user=autonomousvision&repo=unimatch&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
                A unified dense correspondence matching formulation enables three motion and 3D perception tasks to be solved with a unified model.
              </p>
            </td>
          </tr>

          <!-- GMFlow -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/gmflow.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/haofeixu/gmflow">
                <papertitle>GMFlow: Learning Optical Flow via Global Matching</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=9jH5v74AAAAJ">Jing Zhang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a>,
              <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ">Dacheng Tao</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2111.13680">paper</a>
              /
              <a href="./slides/20220413_monash_gmflow.pdf">slides</a>
              /
              <a href="https://www.bilibili.com/video/BV18A4y1R7PL">video(cn)</a>
              /
              <a href="./slides/cvpr2022_gmflow_poster.pdf">poster</a>
              /
              <a href="https://github.com/haofeixu/gmflow">code</a>
              <iframe src="https://ghbtns.com/github-btn.html?user=haofeixu&repo=gmflow&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
              Learning cross-view features with a Transformer enables optical flow to be solved by directly comparing feature similarities.
              </p>
            </td>
          </tr>

          <!-- Flow1D -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/flow1d.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.13918">
                <papertitle>High-Resolution Optical Flow from 1D Attention and Correlation</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://jlyang.org/">Jiaolong Yang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2104.13918">paper</a>
              /
              <a href="https://github.com/haofeixu/flow1d">code</a>
              <p></p>
              <p>
              Factorizing 2D optical flow with 1D attention and 1D correlation enables 4K resolution optical flow estimation on standard GPUs.
              </p>
            </td>
          </tr>

          <!-- RMANet -->
          <!-- <tr">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/rmanet.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12104">
                <papertitle>Recurrent Multi-view Alignment Network for Unsupervised Surface Registration</papertitle>
              </a>
              <br>
              <a href="https://github.com/WanquanF">Wanquan Feng</a>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>,
              <a href="https://rainbowrui.github.io/">Hongrui Cai</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://sites.google.com/site/junhuihoushomepage/">Junhui Hou</a>,
              <a href="https://scholar.google.com/citations?user=AZCcDmsAAAAJ">Hujun Bao</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://wanquanf.github.io/RMA-Net.html">project page</a>
              /
              <a href="https://github.com/WanquanF/RMA-Net">code</a>
              <p></p>
              <p>
              A new non-rigid representation and a differentiable loss function enable end-to-end learning of non-rigid registration.
              </p>
            </td>
          </tr> -->

          <!-- AANet -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/aanet.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/haofeixu/aanet">
                <papertitle>AANet: Adaptive Aggregation Network for Efficient Stereo Matching</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2004.09548">paper</a>
              /
              <a href="https://github.com/haofeixu/aanet">code</a>
              <iframe src="https://ghbtns.com/github-btn.html?user=haofeixu&repo=aanet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
              <p></p>
              <p>
              A sparse points-based cost aggregation method leads to an efficient and accurate stereo matching architecture without any 3D convolutions.
              </p>
            </td>
          </tr>

          <!-- RDN4Depth -->
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/rdn4depth.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.09907">
                <papertitle>Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=sGCf2k0AAAAJ">Jianmin Zheng</a>,
							<a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>
              <br>
              <em>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, 2019
              <br>
              <a href="https://github.com/haofeixu/rdn4depth">code</a>
              <p></p>
              <p>
              A bicubic motion representation enables unsupervised depth estimation from monocular videos in dynamic scenes.
              </p>
            </td>
          </tr> -->


        <!-- Invited Talks -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Invited Talks</heading>
            <ul>
              <li>Learning to Splat, <em>Huawei</em>, Jun 3, 2025</li>
              <li>DepthSplat: Connecting Gaussian Splatting and Depth, <em>Google DeepMind</em>, hosted by <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>, Oct 29, 2024</li>
              <li>Unifying Flow, Stereo and Depth Estimation <a href="./slides/20221228_synced_unimatch.pdf">[slides]</a>,
                <em>Synced</em>, Dec 28, 2022</li>
              <li>GMFlow: Learning Optical Flow via Global Matching <a href="./slides/20220413_monash_gmflow.pdf">[slides]</a>, <em>Monash University</em>, Apr 13, 2022</li>
            </ul>
          </td>
          </tr>
        </tbody></table>

        <!-- Teaching -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            <ul>
              <li>Head Teaching Assistant, <a href="https://cvg.ethz.ch/lectures/3D-vision/">3D Vision</a>, Spring 2025</li>
              <li>Teaching Assistant, <a href="https://cvg.ethz.ch/lectures/Computer-Vision/">Computer Vision</a>, Fall 2024</li>
              <li>Teaching Assistant, <a href="https://video.ethz.ch/lectures/d-infk/2024/spring/252-0870-00L.html">Stochastics and Machine Learning</a>, Spring 2024</li>
            </ul>
          </td>
          </tr>
        </tbody></table>

        <!-- Academic Services -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <ul>
              <li>Conference Reviewer: ICCV 2021, CVPR 2022, ECCV 2022, CVPR 2023, NeurIPS 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, CVPR 2025, ICCV 2025, NeurIPS 2025</li>
              <li>Journal Reviewer: TIP, IJCV, TPAMI</li>
            </ul>
          </td>
          </tr>
        </tbody></table>


        <!-- Awards and Honors -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <ul>
              <li><a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025">Apple Scholar in AI/ML</a>, 2025</li>
              <li><a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewer</a>, NeurIPS 2024</li>
              <li><a href="https://cvpr2022.thecvf.com/outstanding-reviewers">Outstanding Reviewer</a>, CVPR 2022</li>
              <li><a href="https://eval.ai/web/challenges/challenge-page/1704/leaderboard/4066">1<sup>st</sup> place of Argoverse Stereo Challenge</a>, <a href="http://cvpr2022.wad.vision/"> CVPR 2022 Workshop on Autonomous Driving</a></li>
              <li>National Scholarship, 2016</li>
            </ul>
          </td>
          </tr>
        </tbody></table>


        <!-- footnote -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <a href="https://jonbarron.info/"><font size="2", color="gray">Thank Jon Barron for the website's source code</font></a>
              <br>
            </font>
            </p>
            </td>
          </tr>
          </table>

      </td>
    </tr>
  </table>

  




</body>

</html>
