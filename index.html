<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haofei Xu</title>
  
  <meta name="author" content="Haofei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="assets/fly.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- Bio -->
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haofei Xu</name>
              </p>
              <p>
                I am a PhD student at <a href="https://ethz.ch/en.html">ETH Zurich</a> and <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/">University of TÃ¼bingen</a>, supervised by <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a> and <a href="http://www.cvlibs.net/">Andreas Geiger</a>. I have broad interests in computer vision, particularly in dense correspondences, motion and 3D scene representation learning.
              </p>
              <p>
                I was very fortunate to work with several amazing supervisors. I spent a wonderful time working remotely with <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a> at <a href="https://www.monash.edu/">Monash University</a>, Australia, prior to starting my PhD. I obtained a master's degree at <a href="http://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a> supervised by <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>. During my master's, I conducted research at <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a>, Singapore, where I was supervised by <a href="https://jianfei-cai.github.io/">Jianfei Cai</a> and <a href="https://scholar.google.com/citations?user=sGCf2k0AAAAJ">Jianmin Zheng</a>. I also interned at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>, where I was mentored by <a href="https://jlyang.org/">Jiaolong Yang</a> and <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:haofei.xu@inf.ethz.ch">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NhUwq_8AAAAJ">Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/haofeixu">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/haofeixu">Github</a>
              </p>
            </td>
            <td style="padding: 10% 2% 10% 2%;width:40%;max-width:40%">
              <img src="assets/haofeixu.jpg" width="200" alt="photo">
            </td>
          </tr>

        </tbody></table>
        

        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <!-- <p> &#42; denotes equal contribution and <sup>&dagger;</sup> denotes equal advising</p> -->
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- MVSplat -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/mvsplat.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.14627">
                <papertitle>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images
                </papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 
              <br>
              <a href="https://donydchen.github.io/mvsplat/">project page</a>
              /
              <a href="https://github.com/donydchen/mvsplat">code</a>
              <p></p>
              <p>
                A cost volume representation for efficiently predicting 3D Gaussians from sparse multi-view images in a single forward pass.
              </p>
            </td>
          </tr>

          <!-- LaRa -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/lara.gif' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.04699">
                <papertitle>LaRa: Efficient Large-Baseline Radiance Fields
                </papertitle>
              </a>
              <br>
              <a href="https://apchenstu.github.io/">Anpei Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://s-esposito.github.io/">Stefano Esposito</a>,
              <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 
              <br>
              <a href="https://apchenstu.github.io/LaRa/">project page</a>
              /
              <a href="https://github.com/autonomousvision/LaRa">code</a>
              <p></p>
              <p>
                A feed-forward 2DGS model trained in two days using four GPUs.
              </p>
            </td>
          </tr>

          <!-- MuRF -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/murf.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.04565">
                <papertitle>MuRF: Multi-Baseline Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://apchenstu.github.io/">Anpei Chen</a>,
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <a href="https://people.ee.ethz.ch/~csakarid/">Christos Sakaridis</a>,
              <a href="https://yulunzhang.com/">Yulun Zhang</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a><sup>&dagger;</sup>,
              <a href="https://www.yf.io/">Fisher Yu</a><sup>&dagger;</sup>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 
              <br>
              <a href="https://haofeixu.github.io/murf/">project page</a>
              /
              <a href="https://github.com/autonomousvision/murf">code</a>
              <p></p>
              <p>
                A general feed-forward approach to solving sparse view synthesis under multiple different baseline settings.
              </p>
            </td>
          </tr>

          <!-- GoMVS -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/gomvs.jpg' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.07992">
                <papertitle>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</papertitle>
              </a>
              <br>
              <a href="https://github.com/Wuuu3511">Jiang Wu</a>&#42;,
              <a href="https://ruili3.github.io/">Rui Li</a>&#42;,
              <strong>Haofei Xu</strong>,
              Wenxun Zhao,
              Yu Zhu,
              Jinqiu Sun,
              Yanning Zhang
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 
              <br>
              <a href="https://wuuu3511.github.io/gomvs/">project page</a>
              /
              <a href="https://github.com/Wuuu3511/GoMVS">code</a>
              <p></p>
              <p>
                Geometrically consistent matching cost aggregation with monocular normals. <br>
                <font color="red"><strong>1<sup>st</sup> place</strong></font> on <a href="https://www.tanksandtemples.org/leaderboard/AdvancedF/">Tanks and Temples (Advanced) leaderboard</a>.
              </p>
            </td>
          </tr>

          <!-- MatchNeRF -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/matchnerf.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2304.12294">
                <papertitle>Explicit Correspondence Matching for Generalizable Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://donydchen.github.io/">Yuedong Chen</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://wuqianyi.top/">Qianyi Wu</a>,
              <a href="https://www.chuanxiaz.com/">Chuanxia Zheng</a>,
              <a href="https://personal.ntu.edu.sg/astjcham/">Tat-Jen Cham</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>
              <br>
              <em>arXiv</em>, 2023 
              <br>
              <a href="https://donydchen.github.io/matchnerf/">project page</a>
              /
              <a href="https://github.com/donydchen/matchnerf">code</a>
              <p></p>
              <p>
                Employing explicit correspondence matching as a geometry prior enables NeRF to generalize across scenes.
              </p>
            </td>
          </tr>

          <!-- UniMatch -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/unimatch.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2211.05783">
                <papertitle>Unifying Flow, Stereo and Depth Estimation</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=9jH5v74AAAAJ">Jing Zhang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a>,
              <a href="https://www.yf.io/">Fisher Yu</a>,
              <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ">Dacheng Tao</a>,
              <a href="http://www.cvlibs.net/">Andreas Geiger</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2023 
              <br>
              <a href="https://haofeixu.github.io/unimatch/">project page</a>
              /
              <a href="./slides/20221228_synced_unimatch.pdf">slides</a>
              /
              <a href="https://www.bilibili.com/video/BV1uG4y1y7ms">video(cn)</a>
              /
              <a href="https://github.com/autonomousvision/unimatch">code</a>
              /
              <a href="https://colab.research.google.com/drive/1r5m-xVy3Kw60U-m5VB-aQ98oqqg_6cab?usp=sharing">colab</a>
              /
              <a href="https://huggingface.co/spaces/haofeixu/unimatch">demo</a>
              <p></p>
              <p>
                A unified dense correspondence matching formulation enables three motion and 3D perception tasks to be solved with a unified model.
              </p>
            </td>
          </tr>

          <!-- GMFlow -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/gmflow.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.13680">
                <papertitle>GMFlow: Learning Optical Flow via Global Matching</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=9jH5v74AAAAJ">Jing Zhang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="https://scholar.google.com/citations?user=VxAuxMwAAAAJ">Hamid Rezatofighi</a>,
              <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ">Dacheng Tao</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="./slides/20220413_monash_gmflow.pdf">slides</a>
              /
              <a href="https://www.bilibili.com/video/BV18A4y1R7PL">video(cn)</a>
              /
              <a href="./slides/cvpr2022_gmflow_poster.pdf">poster</a>
              /
              <a href="https://github.com/haofeixu/gmflow">code</a>
              <p></p>
              <p>
              Learning strong features with a Transformer enables optical flow to be obtained by directly comparing feature similarities.
              </p>
            </td>
          </tr>

          <!-- Flow1D -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/flow1d.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.13918">
                <papertitle>High-Resolution Optical Flow from 1D Attention and Correlation</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://jlyang.org/">Jiaolong Yang</a>,
              <a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/haofeixu/flow1d">code</a>
              <p></p>
              <p>
              Factorizing 2D optical flow with 1D attention and 1D correlation enables 4K resolution optical flow estimation on ordinary GPUs.
              </p>
            </td>
          </tr>

          <!-- RMANet -->
          <tr">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/rmanet.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12104">
                <papertitle>Recurrent Multi-view Alignment Network for Unsupervised Surface Registration</papertitle>
              </a>
              <br>
              <a href="https://github.com/WanquanF">Wanquan Feng</a>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>,
              <a href="https://rainbowrui.github.io/">Hongrui Cai</a>,
              <strong>Haofei Xu</strong>,
              <a href="https://sites.google.com/site/junhuihoushomepage/">Junhui Hou</a>,
              <a href="https://scholar.google.com/citations?user=AZCcDmsAAAAJ">Hujun Bao</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://wanquanf.github.io/RMA-Net.html">project page</a>
              /
              <a href="https://github.com/WanquanF/RMA-Net">code</a>
              <p></p>
              <p>
              A new non-rigid representation and a differentiable loss function enable end-to-end learning of non-rigid registration.
              </p>
            </td>
          </tr>

          <!-- AANet -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/aanet.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.09548">
                <papertitle>AANet: Adaptive Aggregation Network for Efficient Stereo Matching</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="https://github.com/haofeixu/aanet">code</a>
              <p></p>
              <p>
              A sparse points-based cost aggregation method leads to an efficient and accurate stereo matching architecture without any 3D convolutions.
              </p>
            </td>
          </tr>

          <!-- RDN4Depth -->
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/rdn4depth.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.09907">
                <papertitle>Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos</papertitle>
              </a>
              <br>
              <strong>Haofei Xu</strong>,
              <a href="https://scholar.google.com/citations?user=sGCf2k0AAAAJ">Jianmin Zheng</a>,
							<a href="https://jianfei-cai.github.io/">Jianfei Cai</a>,
              <a href="http://staff.ustc.edu.cn/~juyong">Juyong Zhang</a>
              <br>
              <em>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, 2019
              <br>
              <a href="https://github.com/haofeixu/rdn4depth">code</a>
              <p></p>
              <p>
              A bicubic motion representation enables unsupervised depth estimation from monocular videos in dynamic scenes.
              </p>
            </td>
          </tr> -->


        <!-- Invited Talks -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Invited Talks</heading>
            <ul>
              <li>Unifying Flow, Stereo and Depth Estimation <a href="./slides/20221228_synced_unimatch.pdf">[slides]</a> <a href="https://www.bilibili.com/video/BV1uG4y1y7ms">[video(cn)]</a>,
                <em>æºå¨ä¹å¿</em>, 2022.12.28</li>
              <li>GMFlow: Learning Optical Flow via Global Matching <a href="./slides/20220413_monash_gmflow.pdf">[slides]</a>, <em>Monash University</em>, 2022.04.13</li>
            </ul>
          </td>
          </tr>
        </tbody></table>


        <!-- Academic Services -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <ul>
              <li>Conference Reviewer: ICCV 2021, CVPR 2022, ECCV 2022, CVPR 2023, NeurIPS 2023, CVPR 2024, ECCV 2024</li>
              <li>Journal Reviewer: TIP, IJCV, TPAMI</li>
            </ul>
          </td>
          </tr>
        </tbody></table>


        <!-- Awards and Honors -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <ul>
              <li><a href="https://cvpr2022.thecvf.com/outstanding-reviewers">Outstanding Reviewer</a>, CVPR 2022</li>
              <li><a href="https://eval.ai/web/challenges/challenge-page/1704/leaderboard/4066">1<sup>st</sup> place of Argoverse Stereo Challenge</a>, <a href="http://cvpr2022.wad.vision/"> CVPR 2022 Workshop on Autonomous Driving</a></li>
              <li>National Scholarship, 2016</li>
            </ul>
          </td>
          </tr>
        </tbody></table>


        <!-- footnote -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="center">
              <font size="2">
              <a href="https://jonbarron.info/"><font size="2" color="lightgray">awesome website template</font></a>
              <br>
            </font>
            </p>
            </td>
          </tr>
          </table>

      </td>
    </tr>
  </table>

  




</body>

</html>
